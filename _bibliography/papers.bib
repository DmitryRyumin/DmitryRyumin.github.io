---
---

@string{ESWA = {Expert Systems with Applications}}

@article{Ruimina2024OCEAN-AI,
  abbr={ESWA},
  author = {Elena Ryumina and Maxim Markitantov and Dmitry Ryumin and Alexey Karpov},
  title = {OCEAN-AI Framework with EmoFormer Cross-Hemiface Attention Approach for Personality Traits Assessment},
  journal = ESWA,
  volume = {239},
  pages = {122441},
  year = {2024},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2023.122441},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417423029433},
  abstract = {Psychological and neurological studies earlier suggested that a personality type can be determined by the whole face as well as by its sides. This article discusses novel research using deep neural networks that address the features of both sides of the face (hemifaces) to assess the human’s Big Five personality traits (PT). For this, we have developed a real-time approach called EmoFormer with cross-hemiface attention. The novelty of the presented approach lies in the confirmation that each hemiface exhibits high predictive capabilities in terms of human’s PT distinction. Our approach is based on a novel mid-level emotional feature extractor for each hemiface and a cross-hemiface attention fusion strategy for hemiface feature aggregation. The consequent fusion of both hemifaces has outperformed the use of the whole face by the relative value of 3.6% in terms of Concordance Correlation Coefficient (0.634 vs. 0.612) on the ChaLearn First Impressions V2 corpus. The proposed approach has also outperformed all the existing state-of-the-art approaches for PT assessment based on the face modality. We have also analyzed the “best hemiface”, the one that predicts PT more accurately in terms of demographic characteristics (gender, ethnicity, and age). We have found that the best hemiface for two of the five PT (Openness to experience and Non-Neuroticism) is different depending on demographic characteristics. For the other three traits, the right hemiface is dominant for Extraversion, while the left one is more indicative of Conscientiousness and Agreeableness. These findings support previous psychological and neurological research. Besides, we provide an open-source framework referred to as OCEAN-AI that can be seamlessly integrated into expert systems with practical applications in various domains including healthcare, education, and human resources.},
  keywords = {Personality Computing, Big Five, Emotional Features, Hemifaces, Feature-Level Fusion, Deep Learning, Transformer},
  html={https://www.sciencedirect.com/science/article/pii/S0957417423029433},
  bibtex_show={true},
  google_scholar_id={UxriW0iASnsC},
  selected={true}
}


@Article{Ryumin2024s23042284,
  abbr={Sensors},
  author = {Dmitry Ryumin and Denis Ivanko and Elena Ryumina},
  title = {Audio-Visual Speech and Gesture Recognition by Sensors of Mobile Devices},
  journal = {Sensors},
  volume = {23},
  number = {4},
  article-number = {2284},
  year = {2023},
  issn = {1424-8220},
  doi = {10.3390/s23042284},
  url = {https://www.mdpi.com/1424-8220/23/4/2284},
  abstract = {Audio-visual speech recognition (AVSR) is one of the most promising solutions for reliable speech recognition, particularly when audio is corrupted by noise. Additional visual information can be used for both automatic lip-reading and gesture recognition. Hand gestures are a form of non-verbal communication and can be used as a very important part of modern human–computer interaction systems. Currently, audio and video modalities are easily accessible by sensors of mobile devices. However, there is no out-of-the-box solution for automatic audio-visual speech and gesture recognition. This study introduces two deep neural network-based model architectures: one for AVSR and one for gesture recognition. The main novelty regarding audio-visual speech recognition lies in fine-tuning strategies for both visual and acoustic features and in the proposed end-to-end model, which considers three modality fusion approaches: prediction-level, feature-level, and model-level. The main novelty in gesture recognition lies in a unique set of spatio-temporal features, including those that consider lip articulation information. As there are no available datasets for the combined task, we evaluated our methods on two different large-scale corpora—LRW and AUTSL—and outperformed existing methods on both audio-visual speech recognition and gesture recognition tasks. We achieved AVSR accuracy for the LRW dataset equal to 98.76% and gesture recognition rate for the AUTSL dataset equal to 98.56%. The results obtained demonstrate not only the high performance of the proposed methodology, but also the fundamental possibility of recognizing audio-visual speech and gestures by sensors of mobile devices.},
  keywords = {Audio-Visual Speech Recognition, Model-Level Fusion, Lip-Reading, Gesture Recognition, Spatio-Temporal Features, Dimensionality Reduction Technique, Computer Vision},
  html={https://www.mdpi.com/1424-8220/23/4/2284},
  bibtex_show={true},
  google_scholar_id={NhqRSupF_l8C},
  dimensions={true},
  selected={true}
}
