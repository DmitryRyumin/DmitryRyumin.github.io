---
---

@string{ESWA = {Expert Systems with Applications}}
@string{INTERSPEECH = {Proceedings of ISCA International Conference INTERSPEECH}}

@article{Ruimina2024OCEAN-AI,
  abbr = {ESWA},
  author = {Elena Ryumina and Maxim Markitantov and Dmitry Ryumin and Alexey Karpov},
  title = {{OCEAN-AI Framework with EmoFormer Cross-Hemiface Attention Approach for Personality Traits Assessment}},
  journal = ESWA,
  volume = {239},
  pages = {122441},
  year = {2024},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2023.122441},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417423029433},
  abstract = {Psychological and neurological studies earlier suggested that a personality type can be determined by the whole face as well as by its sides. This article discusses novel research using deep neural networks that address the features of both sides of the face (hemifaces) to assess the human’s Big Five personality traits (PT). For this, we have developed a real-time approach called EmoFormer with cross-hemiface attention. The novelty of the presented approach lies in the confirmation that each hemiface exhibits high predictive capabilities in terms of human’s PT distinction. Our approach is based on a novel mid-level emotional feature extractor for each hemiface and a cross-hemiface attention fusion strategy for hemiface feature aggregation. The consequent fusion of both hemifaces has outperformed the use of the whole face by the relative value of 3.6% in terms of Concordance Correlation Coefficient (0.634 vs. 0.612) on the ChaLearn First Impressions V2 corpus. The proposed approach has also outperformed all the existing state-of-the-art approaches for PT assessment based on the face modality. We have also analyzed the “best hemiface”, the one that predicts PT more accurately in terms of demographic characteristics (gender, ethnicity, and age). We have found that the best hemiface for two of the five PT (Openness to experience and Non-Neuroticism) is different depending on demographic characteristics. For the other three traits, the right hemiface is dominant for Extraversion, while the left one is more indicative of Conscientiousness and Agreeableness. These findings support previous psychological and neurological research. Besides, we provide an open-source framework referred to as OCEAN-AI that can be seamlessly integrated into expert systems with practical applications in various domains including healthcare, education, and human resources.},
  keywords = {Personality Computing, Big Five, Emotional Features, Hemifaces, Feature-Level Fusion, Deep Learning, Transformer},
  html = {https://www.sciencedirect.com/science/article/pii/S0957417423029433},
  bibtex_show = {true},
  google_scholar_id = {UxriW0iASnsC},
  preview = {Ruimina2024OCEAN-AI.jpg},
  selected = {true}
}

@article{Ryumin2024s23042284,
  abbr = {Sensors},
  author = {Dmitry Ryumin and Denis Ivanko and Elena Ryumina},
  title = {{Audio-Visual Speech and Gesture Recognition by Sensors of Mobile Devices}},
  journal = {Sensors},
  volume = {23},
  number = {4},
  article-number = {2284},
  year = {2023},
  issn = {1424-8220},
  doi = {10.3390/s23042284},
  url = {https://www.mdpi.com/1424-8220/23/4/2284},
  abstract = {Audio-visual speech recognition (AVSR) is one of the most promising solutions for reliable speech recognition, particularly when audio is corrupted by noise. Additional visual information can be used for both automatic lip-reading and gesture recognition. Hand gestures are a form of non-verbal communication and can be used as a very important part of modern human–computer interaction systems. Currently, audio and video modalities are easily accessible by sensors of mobile devices. However, there is no out-of-the-box solution for automatic audio-visual speech and gesture recognition. This study introduces two deep neural network-based model architectures: one for AVSR and one for gesture recognition. The main novelty regarding audio-visual speech recognition lies in fine-tuning strategies for both visual and acoustic features and in the proposed end-to-end model, which considers three modality fusion approaches: prediction-level, feature-level, and model-level. The main novelty in gesture recognition lies in a unique set of spatio-temporal features, including those that consider lip articulation information. As there are no available datasets for the combined task, we evaluated our methods on two different large-scale corpora—LRW and AUTSL—and outperformed existing methods on both audio-visual speech recognition and gesture recognition tasks. We achieved AVSR accuracy for the LRW dataset equal to 98.76% and gesture recognition rate for the AUTSL dataset equal to 98.56%. The results obtained demonstrate not only the high performance of the proposed methodology, but also the fundamental possibility of recognizing audio-visual speech and gestures by sensors of mobile devices.},
  keywords = {Audio-Visual Speech Recognition, Model-Level Fusion, Lip-Reading, Gesture Recognition, Spatio-Temporal Features, Dimensionality Reduction Technique, Computer Vision},
  html = {https://www.mdpi.com/1424-8220/23/4/2284},
  bibtex_show = {true},
  google_scholar_id = {NhqRSupF_l8C},
  preview = {Ryumin2024s23042284.png},
  dimensions = {true},
  selected = {true}
}

@article{Kashevnik2021_9364986,
  abbr = {IEEE Access},
  author = {Alexey Kashevnik and Igor Lashkov and Alexandr Axyonov and Denis Ivanko and Dmitry Ryumin and Artem Kolchin and Alexey Karpov},
  title = {{Multimodal Corpus Design for Audio-Visual Speech Recognition in Vehicle Cabin}},
  journal = {IEEE Access},
  volume = {9},
  number = {},
  year = {2021},
  pages = {34986--35003},
  ISSN = {2169-3536},
  doi = {10.1109/ACCESS.2021.3062752},
  url = {https://ieeexplore.ieee.org/document/9364986},
  abstract = {This paper introduces a new methodology aimed at comfort for the driver in-the-wild multimodal corpus creation for audio-visual speech recognition in driver monitoring systems. The presented methodology is universal and can be used for corpus recording for different languages. We present an analysis of speech recognition systems and voice interfaces for driver monitoring systems based on the analysis of both audio and video data. Multimodal speech recognition allows using audio data when video data are useless (e.g. at nighttime), as well as applying video data in acoustically noisy conditions (e.g., at highways). Our methodology identifies the main steps and requirements for multimodal corpus designing, including the development of a new framework for audio-visual corpus creation. We identify the main research questions related to the speech corpus creation task and discuss them in detail in this paper. We also consider some main cases of usage that require speech recognition in a vehicle cabin for interaction with a driver monitoring system. We also consider other important use cases when the system detects dangerous states of driver's drowsiness and starts a question-answer game to prevent dangerous situations. At the end based on the proposed methodology, we developed a mobile application that allows us to record a corpus for the Russian language. We created RUSAVIC corpus using the developed mobile application that at the moment a unique audiovisual corpus for the Russian language that is recorded in-the-wild condition.},
  keywords = {Vehicles, Speech Recognition, Smart Phones, Monitoring, Sensors, Vocabulary, Task Analysis, Driver Monitoring, Automatic Speech Recognition, Multimodal Corpus, Human–Computer Interaction},
  html = {https://ieeexplore.ieee.org/document/9364986},
  pdf = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9364986},
  bibtex_show = {true},
  google_scholar_id = {_FxGoFyzp5QC},
  preview = {Kashevnik2021_9364986.png},
  dimensions = {true},
  selected = {true}
}

@inproceedings{ryumina23_interspeech,
  abbr = {Interspeech},
  author = {Elena Ryumina and Dmitry Ryumin and Maxim Markitantov and Heysem Kaya and Alexey Karpov},
  title = {{Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech}},
  booktitle = INTERSPEECH,
  year = {2023},
  pages = {4049--4053},
  doi = {10.21437/Interspeech.2023-1686},
  url = {https://www.isca-archive.org/interspeech_2023/ryumina23_interspeech.html},
  abstract = {Automatic personality traits assessment (PTA) provides high-level, intelligible predictive inputs for subsequent critical downstream tasks, such as job interview recommendations and mental healthcare monitoring. In this work, we introduce a novel Multimodal Personality Traits Assessment (MuPTA) corpus. Our MuPTA corpus is unique in that it contains both spontaneous and read speech collected in the midly-resourced Russian language. We present a novel audio-visual approach for PTA that is used in order to set up baseline results on this corpus. We further analyze the impact of both spontaneous and read speech types on the PTA predictive performance. We find that for the audio modality, the PTA predictive performances on short signals are almost equal regardless of the speech type, while PTA using video modality is more accurate with spontaneous speech compared to read one regardless of the signal length.},
  keywords = {Audio-Visual Resources, Data Annotation, Multimodal Paralinguistics, Personality Computing, Big Five Traits},
  html = {https://www.isca-archive.org/interspeech_2023/ryumina23_interspeech.html},
  pdf={https://www.isca-archive.org/interspeech_2023/ryumina23_interspeech.pdf},
  bibtex_show = {true},
  google_scholar_id = {KxtntwgDAa4C},
  preview = {ryumina23_interspeech.png},
  dimensions = {true},
  selected = {true}
}